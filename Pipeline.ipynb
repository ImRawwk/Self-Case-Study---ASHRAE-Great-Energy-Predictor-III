{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing essential libraries\n",
    "\n",
    "import random\n",
    "import math \n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgbm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition function for imputing missing weather values \n",
    "\n",
    "'''This function removes null values present in the weather dataframe'''\n",
    "\n",
    "def fill_weather_dataset(weather_df):\n",
    "    \n",
    "    # Find Missing Dates\n",
    "    time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_date = datetime.datetime.strptime(weather_df['timestamp'].min(),time_format)\n",
    "    end_date = datetime.datetime.strptime(weather_df['timestamp'].max(),time_format)\n",
    "    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n",
    "    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n",
    "\n",
    "    missing_hours = []\n",
    "    for site_id in range(16):\n",
    "        site_hours = np.array(weather_df[weather_df['site_id'] == site_id]['timestamp'])\n",
    "        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n",
    "        new_rows['site_id'] = site_id\n",
    "        weather_df = pd.concat([weather_df,new_rows])\n",
    "\n",
    "        weather_df = weather_df.reset_index(drop=True)           \n",
    "\n",
    "    # Add new Features\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n",
    "    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n",
    "    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n",
    "    \n",
    "    # Reset Index for Fast Update\n",
    "    weather_df = weather_df.set_index(['site_id','day','month'])\n",
    "\n",
    "    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "    weather_df.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n",
    "    # Step 2\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n",
    "\n",
    "    weather_df.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "    due_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "    weather_df.update(due_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n",
    "    # Step 2\n",
    "    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n",
    "\n",
    "    weather_df.update(sea_level_filler,overwrite=False)\n",
    "\n",
    "    wind_direction_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n",
    "    weather_df.update(wind_direction_filler,overwrite=False)\n",
    "\n",
    "    wind_speed_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n",
    "    weather_df.update(wind_speed_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n",
    "    # Step 2\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "\n",
    "    weather_df.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "    weather_df = weather_df.reset_index()\n",
    "    weather_df = weather_df.drop(['datetime','day','week','month'],axis=1)\n",
    "        \n",
    "    return weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This function adds lags feature in the weather dataframe for given window '''\n",
    "\n",
    "def add_lag_feature(weather_df, window=3):\n",
    "        group_df = weather_df.groupby('site_id')\n",
    "        cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n",
    "        rolled = group_df[cols].rolling(window=window, min_periods=0)\n",
    "        lag_mean = rolled.mean().reset_index().astype(np.float16)\n",
    "        lag_max = rolled.max().reset_index().astype(np.float16)\n",
    "        lag_min = rolled.min().reset_index().astype(np.float16)\n",
    "        lag_std = rolled.std().reset_index().astype(np.float16)\n",
    "        for col in cols:\n",
    "            weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n",
    "            weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n",
    "            weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n",
    "            weather_df[f'{col}_std_lag{window}'] = lag_std[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_1(X_input):\n",
    "    \n",
    "    build_meta = pd.read_csv('building_metadata.csv')\n",
    "    weather_tr = pd.read_csv('weather_train.csv')\n",
    "    weather_te = pd.read_csv('weather_test.csv')\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    test_data = X_input\n",
    "    \n",
    "    \n",
    "    train_data = train_data[150000:150100]\n",
    "    \n",
    "    #Preprocessing weather dataframes for imputing missing values \n",
    "    \n",
    "    #preprocessing weather train data \n",
    "    weather_tr[\"cloud_coverage\"] = (weather_tr[\"cloud_coverage\"]).astype(np.float32)\n",
    "    weather_tr[\"sea_level_pressure\"] = (weather_tr[\"sea_level_pressure\"]).astype(np.float32)\n",
    "    weather_tr[\"precip_depth_1_hr\"] = (weather_tr[\"precip_depth_1_hr\"]).astype(np.float32)\n",
    "    \n",
    "    #preprocessing weather test data\n",
    "    weather_te[\"cloud_coverage\"] = (weather_te[\"cloud_coverage\"]).astype(np.float32)\n",
    "    weather_te[\"sea_level_pressure\"] = (weather_te[\"sea_level_pressure\"]).astype(np.float32)\n",
    "    weather_te[\"precip_depth_1_hr\"] = (weather_te[\"precip_depth_1_hr\"]).astype(np.float32)\n",
    "    \n",
    "    weather_train_df = fill_weather_dataset(weather_tr)\n",
    "    weather_test_df = fill_weather_dataset(weather_te)\n",
    "    \n",
    "    \n",
    "   #Performing a part of feature engineering related to weather data so as to avoid merging issues\n",
    "       \n",
    "            \n",
    "    add_lag_feature(weather_train_df, window=3)\n",
    "    add_lag_feature(weather_train_df, window=72)\n",
    "    \n",
    "    add_lag_feature(weather_test_df, window=3)\n",
    "    add_lag_feature(weather_test_df, window=72)\n",
    "    \n",
    "    #Merge the dataframes\n",
    "    train_one = train_data.merge(right = build_meta, on = 'building_id', how = 'left')\n",
    "    xtr = train_one.merge(right = weather_train_df, on = ['site_id','timestamp'], how = 'left')\n",
    "    \n",
    "    test_one = test_data.merge(right = build_meta, on = 'building_id', how = 'left')\n",
    "    xte = test_one.merge(right = weather_test_df, on = ['site_id','timestamp'], how = 'left')\n",
    "    \n",
    "    final_xtr = xtr\n",
    "    final_xte = xte\n",
    "    \n",
    "    #Feature Engineering \n",
    "                     \n",
    "    final_xtr[\"timestamp\"] = pd.to_datetime(final_xtr[\"timestamp\"])\n",
    "    final_xtr[\"year\"] = final_xtr[\"timestamp\"].dt.year\n",
    "    final_xtr[\"month\"] = final_xtr[\"timestamp\"].dt.month\n",
    "    final_xtr[\"weekend\"] = final_xtr[\"timestamp\"].dt.weekday\n",
    "    final_xtr[\"day\"] = final_xtr[\"timestamp\"].dt.day\n",
    "    final_xtr[\"hour\"] = final_xtr[\"timestamp\"].dt.hour\n",
    "    final_xtr['log_square_feet'] = np.log(final_xtr['square_feet'])\n",
    "    \n",
    "    final_xte[\"timestamp\"] = pd.to_datetime(final_xte[\"timestamp\"])\n",
    "    final_xte[\"year\"] = final_xte[\"timestamp\"].dt.year\n",
    "    final_xte[\"month\"] = final_xte[\"timestamp\"].dt.month\n",
    "    final_xte[\"weekend\"] = final_xte[\"timestamp\"].dt.weekday\n",
    "    final_xte[\"day\"] = final_xte[\"timestamp\"].dt.day\n",
    "    final_xte[\"hour\"] = final_xte[\"timestamp\"].dt.hour\n",
    "    final_xte['log_square_feet'] = np.log(final_xte['square_feet'])\n",
    "    \n",
    "    # Dropping columna with high missing values & which are not so useful\n",
    "    final_xtr.drop(columns=[\"timestamp\",\"year_built\",\"floor_count\"], inplace=True)\n",
    "    final_xtr.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    final_xte.drop(columns=[\"timestamp\",\"year_built\",\"floor_count\"], inplace=True)\n",
    "    final_xte.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    final_xtr['meter_reading_log1p'] = np.log1p(final_xtr['meter_reading'])\n",
    "    y_train = final_xtr['meter_reading_log1p']\n",
    "    \n",
    "    # Dropping all the engineered weather features with high correlation\n",
    "    final_xtr.drop(columns=['air_temperature_max_lag72', 'air_temperature_min_lag72', \n",
    "        'air_temperature_std_lag72', 'cloud_coverage_std_lag72',\n",
    "        'cloud_coverage_max_lag72', 'cloud_coverage_min_lag72', \n",
    "       'dew_temperature_mean_lag72', 'dew_temperature_max_lag72',\n",
    "       'dew_temperature_std_lag72', 'precip_depth_1_hr_max_lag72',\n",
    "       'precip_depth_1_hr_min_lag72', 'precip_depth_1_hr_std_lag72',\n",
    "       'sea_level_pressure_max_lag72', 'sea_level_pressure_min_lag72',\n",
    "       'wind_direction_min_lag72', 'wind_direction_std_lag72',\n",
    "       'wind_speed_mean_lag72', 'air_temperature_max_lag3',\n",
    "        'air_temperature_min_lag3', 'air_temperature_std_lag3', \n",
    "        'cloud_coverage_std_lag3', 'cloud_coverage_max_lag3', \n",
    "        'cloud_coverage_min_lag3', 'dew_temperature_mean_lag3', \n",
    "        'dew_temperature_max_lag3','dew_temperature_std_lag3',\n",
    "        'precip_depth_1_hr_max_lag3','precip_depth_1_hr_min_lag3', \n",
    "        'precip_depth_1_hr_std_lag3','sea_level_pressure_max_lag3',\n",
    "        'sea_level_pressure_min_lag3','wind_direction_min_lag3', \n",
    "        'wind_direction_std_lag3','wind_speed_mean_lag3' ], inplace=True)\n",
    "    final_xtr.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    final_xte.drop(columns=['air_temperature_max_lag72', 'air_temperature_min_lag72', \n",
    "        'air_temperature_std_lag72', 'cloud_coverage_std_lag72',\n",
    "        'cloud_coverage_max_lag72', 'cloud_coverage_min_lag72', \n",
    "       'dew_temperature_mean_lag72', 'dew_temperature_max_lag72',\n",
    "       'dew_temperature_std_lag72', 'precip_depth_1_hr_max_lag72',\n",
    "       'precip_depth_1_hr_min_lag72', 'precip_depth_1_hr_std_lag72',\n",
    "       'sea_level_pressure_max_lag72', 'sea_level_pressure_min_lag72',\n",
    "       'wind_direction_min_lag72', 'wind_direction_std_lag72',\n",
    "       'wind_speed_mean_lag72', 'air_temperature_max_lag3',\n",
    "        'air_temperature_min_lag3', 'air_temperature_std_lag3', \n",
    "        'cloud_coverage_std_lag3', 'cloud_coverage_max_lag3', \n",
    "        'cloud_coverage_min_lag3', 'dew_temperature_mean_lag3', \n",
    "        'dew_temperature_max_lag3','dew_temperature_std_lag3',\n",
    "        'precip_depth_1_hr_max_lag3','precip_depth_1_hr_min_lag3', \n",
    "        'precip_depth_1_hr_std_lag3','sea_level_pressure_max_lag3',\n",
    "        'sea_level_pressure_min_lag3','wind_direction_min_lag3', \n",
    "        'wind_direction_std_lag3','wind_speed_mean_lag3' ], inplace=True)\n",
    "    final_xte.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    #Drop target feature from the training set\n",
    "    final_xtr = final_xtr.drop(columns=['meter_reading', 'meter_reading_log1p'])\n",
    "        \n",
    "    #Label encoding primary use feature\n",
    "    all_prim= ['Education', 'Entertainment/public assembly', 'Lodging/residential',\n",
    "              'Office', 'Other', 'Parking', 'Retail', 'Office', 'Public services', \n",
    "              'Healthcare', 'Warehouse/storage', 'Manufacturing/Industrial',\n",
    "              'Services', 'Technology/science', 'Food sales and services',\n",
    "              'Utility', 'Religious worship', 'Food sales and service', \n",
    "               'Manufacturing/industrial']\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(all_prim)\n",
    "    final_xtr['primary_use'] = pd.Series(le.transform(final_xtr['primary_use']))\n",
    "    final_xte['primary_use'] = pd.Series(le.transform(final_xte['primary_use']))\n",
    "    \n",
    "    X_train = final_xtr.fillna(0)\n",
    "    X_test = final_xte.fillna(0)\n",
    "    X_test = X_test.drop(columns=['row_id'])\n",
    "    \n",
    "    #Defining model parameters & training the model\n",
    "    params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"num_leaves\": 800,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"reg_lambda\": 5, \n",
    "    \"metric\": \"rmse\",\n",
    "     }\n",
    "    \n",
    "    lgbm_model = lgbm.LGBMRegressor(**params)\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "    \n",
    "    #Predicting on test set\n",
    "    preds = np.expm1(lgbm_model.predict(X_test))\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a test set of 1 point for getting predictions \n",
    "test = pd.read_csv('test.csv') \n",
    "test_data = test[150000:150001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55.90135431])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_fun_1(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is a scorer function that will give a RMSLE score to our predictions '''\n",
    "\n",
    "def rmsle_score(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
    "    return (sum(to_sum) * (1.0/len(y))) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  final_fun_2(X, Y):\n",
    "    \n",
    "    build_meta = pd.read_csv('building_metadata.csv')\n",
    "    weather_tr = pd.read_csv('weather_train.csv')\n",
    "    weather_te = pd.read_csv('weather_test.csv')\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    test_data = X\n",
    "    \n",
    "    \n",
    "    train_data = train_data[150000:150100]\n",
    "    \n",
    "    #Preprocessing weather dataframes for imputing missing values \n",
    "    \n",
    "    #preprocessing weather train data \n",
    "    weather_tr[\"cloud_coverage\"] = (weather_tr[\"cloud_coverage\"]).astype(np.float32)\n",
    "    weather_tr[\"sea_level_pressure\"] = (weather_tr[\"sea_level_pressure\"]).astype(np.float32)\n",
    "    weather_tr[\"precip_depth_1_hr\"] = (weather_tr[\"precip_depth_1_hr\"]).astype(np.float32)\n",
    "    \n",
    "    #preprocessing weather test data\n",
    "    weather_te[\"cloud_coverage\"] = (weather_te[\"cloud_coverage\"]).astype(np.float32)\n",
    "    weather_te[\"sea_level_pressure\"] = (weather_te[\"sea_level_pressure\"]).astype(np.float32)\n",
    "    weather_te[\"precip_depth_1_hr\"] = (weather_te[\"precip_depth_1_hr\"]).astype(np.float32)\n",
    "    \n",
    "    weather_train_df = fill_weather_dataset(weather_tr)\n",
    "    weather_test_df = fill_weather_dataset(weather_te)\n",
    "    \n",
    "    \n",
    "   #Performing a part of feature engineering related to weather data so as to avoid merging issues\n",
    "       \n",
    "            \n",
    "    add_lag_feature(weather_train_df, window=3)\n",
    "    add_lag_feature(weather_train_df, window=72)\n",
    "    \n",
    "    add_lag_feature(weather_test_df, window=3)\n",
    "    add_lag_feature(weather_test_df, window=72)\n",
    "    \n",
    "    #Merge the dataframes\n",
    "    train_one = train_data.merge(right = build_meta, on = 'building_id', how = 'left')\n",
    "    xtr = train_one.merge(right = weather_train_df, on = ['site_id','timestamp'], how = 'left')\n",
    "    \n",
    "    test_one = test_data.merge(right = build_meta, on = 'building_id', how = 'left')\n",
    "    xte = test_one.merge(right = weather_test_df, on = ['site_id','timestamp'], how = 'left')\n",
    "    \n",
    "    final_xtr = xtr\n",
    "    final_xte = xte\n",
    "    \n",
    "    #Feature Engineering \n",
    "                     \n",
    "    final_xtr[\"timestamp\"] = pd.to_datetime(final_xtr[\"timestamp\"])\n",
    "    final_xtr[\"year\"] = final_xtr[\"timestamp\"].dt.year\n",
    "    final_xtr[\"month\"] = final_xtr[\"timestamp\"].dt.month\n",
    "    final_xtr[\"weekend\"] = final_xtr[\"timestamp\"].dt.weekday\n",
    "    final_xtr[\"day\"] = final_xtr[\"timestamp\"].dt.day\n",
    "    final_xtr[\"hour\"] = final_xtr[\"timestamp\"].dt.hour\n",
    "    final_xtr['log_square_feet'] = np.log(final_xtr['square_feet'])\n",
    "    \n",
    "    final_xte[\"timestamp\"] = pd.to_datetime(final_xte[\"timestamp\"])\n",
    "    final_xte[\"year\"] = final_xte[\"timestamp\"].dt.year\n",
    "    final_xte[\"month\"] = final_xte[\"timestamp\"].dt.month\n",
    "    final_xte[\"weekend\"] = final_xte[\"timestamp\"].dt.weekday\n",
    "    final_xte[\"day\"] = final_xte[\"timestamp\"].dt.day\n",
    "    final_xte[\"hour\"] = final_xte[\"timestamp\"].dt.hour\n",
    "    final_xte['log_square_feet'] = np.log(final_xte['square_feet'])\n",
    "    \n",
    "    # Dropping columna with high missing values & which are not so useful\n",
    "    final_xtr.drop(columns=[\"timestamp\",\"year_built\",\"floor_count\"], inplace=True)\n",
    "    final_xtr.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    final_xte.drop(columns=[\"timestamp\",\"year_built\",\"floor_count\"], inplace=True)\n",
    "    final_xte.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    final_xtr['meter_reading_log1p'] = np.log1p(final_xtr['meter_reading'])\n",
    "    y_train = final_xtr['meter_reading_log1p']\n",
    "    \n",
    "    # Dropping all the engineered weather features with high correlation\n",
    "    final_xtr.drop(columns=['air_temperature_max_lag72', 'air_temperature_min_lag72', \n",
    "        'air_temperature_std_lag72', 'cloud_coverage_std_lag72',\n",
    "        'cloud_coverage_max_lag72', 'cloud_coverage_min_lag72', \n",
    "       'dew_temperature_mean_lag72', 'dew_temperature_max_lag72',\n",
    "       'dew_temperature_std_lag72', 'precip_depth_1_hr_max_lag72',\n",
    "       'precip_depth_1_hr_min_lag72', 'precip_depth_1_hr_std_lag72',\n",
    "       'sea_level_pressure_max_lag72', 'sea_level_pressure_min_lag72',\n",
    "       'wind_direction_min_lag72', 'wind_direction_std_lag72',\n",
    "       'wind_speed_mean_lag72', 'air_temperature_max_lag3',\n",
    "        'air_temperature_min_lag3', 'air_temperature_std_lag3', \n",
    "        'cloud_coverage_std_lag3', 'cloud_coverage_max_lag3', \n",
    "        'cloud_coverage_min_lag3', 'dew_temperature_mean_lag3', \n",
    "        'dew_temperature_max_lag3','dew_temperature_std_lag3',\n",
    "        'precip_depth_1_hr_max_lag3','precip_depth_1_hr_min_lag3', \n",
    "        'precip_depth_1_hr_std_lag3','sea_level_pressure_max_lag3',\n",
    "        'sea_level_pressure_min_lag3','wind_direction_min_lag3', \n",
    "        'wind_direction_std_lag3','wind_speed_mean_lag3' ], inplace=True)\n",
    "    final_xtr.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    final_xte.drop(columns=['air_temperature_max_lag72', 'air_temperature_min_lag72', \n",
    "        'air_temperature_std_lag72', 'cloud_coverage_std_lag72',\n",
    "        'cloud_coverage_max_lag72', 'cloud_coverage_min_lag72', \n",
    "       'dew_temperature_mean_lag72', 'dew_temperature_max_lag72',\n",
    "       'dew_temperature_std_lag72', 'precip_depth_1_hr_max_lag72',\n",
    "       'precip_depth_1_hr_min_lag72', 'precip_depth_1_hr_std_lag72',\n",
    "       'sea_level_pressure_max_lag72', 'sea_level_pressure_min_lag72',\n",
    "       'wind_direction_min_lag72', 'wind_direction_std_lag72',\n",
    "       'wind_speed_mean_lag72', 'air_temperature_max_lag3',\n",
    "        'air_temperature_min_lag3', 'air_temperature_std_lag3', \n",
    "        'cloud_coverage_std_lag3', 'cloud_coverage_max_lag3', \n",
    "        'cloud_coverage_min_lag3', 'dew_temperature_mean_lag3', \n",
    "        'dew_temperature_max_lag3','dew_temperature_std_lag3',\n",
    "        'precip_depth_1_hr_max_lag3','precip_depth_1_hr_min_lag3', \n",
    "        'precip_depth_1_hr_std_lag3','sea_level_pressure_max_lag3',\n",
    "        'sea_level_pressure_min_lag3','wind_direction_min_lag3', \n",
    "        'wind_direction_std_lag3','wind_speed_mean_lag3' ], inplace=True)\n",
    "    final_xte.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    #Drop target feature from the training set\n",
    "    final_xtr = final_xtr.drop(columns=['meter_reading', 'meter_reading_log1p'])\n",
    "    final_xte = final_xte.drop(columns=['meter_reading'])   \n",
    "    \n",
    "    #Label encoding primary use feature\n",
    "    all_prim= ['Education', 'Entertainment/public assembly', 'Lodging/residential',\n",
    "              'Office', 'Other', 'Parking', 'Retail', 'Office', 'Public services', \n",
    "              'Healthcare', 'Warehouse/storage', 'Manufacturing/Industrial',\n",
    "              'Services', 'Technology/science', 'Food sales and services',\n",
    "              'Utility', 'Religious worship', 'Food sales and service', \n",
    "               'Manufacturing/industrial']\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(all_prim)\n",
    "    final_xtr['primary_use'] = pd.Series(le.transform(final_xtr['primary_use']))\n",
    "    final_xte['primary_use'] = pd.Series(le.transform(final_xte['primary_use']))\n",
    "    \n",
    "    X_train = final_xtr.fillna(0)\n",
    "    X_test = final_xte.fillna(0)\n",
    "    #X_test = X_test.drop(columns=['row_id'])\n",
    "    \n",
    "    #Defining model parameters & training the model\n",
    "    params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"num_leaves\": 800,\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"reg_lambda\": 5, \n",
    "    \"metric\": \"rmse\",\n",
    "     }\n",
    "    \n",
    "    lgbm_model = lgbm.LGBMRegressor(**params)\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "    \n",
    "    #Predicting on test set\n",
    "    preds = np.expm1(lgbm_model.predict(X_test))\n",
    "    \n",
    "    Test_RMSLE = (rmsle_score(np.expm1(y_test) , preds))\n",
    "    return Test_RMSLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default test set provided in the competition doesn't have y label so we are making a test set from the training set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating test set of 1 point for testing our function & its corresponding y label\n",
    "test = pd.read_csv('train.csv') \n",
    "test_data = test[150700:150701]\n",
    "y = test_data['meter_reading']\n",
    "y_test = np.log1p(y)\n",
    "y_test = y_test.to_frame()\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3631648131211076"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_fun_2(test_data, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
